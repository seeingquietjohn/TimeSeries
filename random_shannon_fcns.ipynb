{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4460b153-0e82-4590-a97f-d72a934e6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log, e\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.stats import linregress\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2924f103-24d5-4485-b340-08b6af3c95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frobnorm(mat): \n",
    "    r\"\"\"\n",
    "        Compute the frobenius norm \n",
    "    \"\"\"\n",
    "    n, m = len(mat), len(mat[0])\n",
    "    matsum = 0\n",
    "    for i in range(0,n): \n",
    "        for j in range(0,m): \n",
    "            matsum += mat[i][j]**2\n",
    "\n",
    "    return matsum**0.5\n",
    "\n",
    "\n",
    "def cossim(a,b): \n",
    "    r\"\"\"\n",
    "        Compute the cossine similarity\n",
    "    \"\"\"\n",
    "    return np.trace(a.T @ b) / (frobnorm(a)*frobnorm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f4df37-9997-40b0-ab4a-58099b5859b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ell_inf(A, B): # chebyshev norm\n",
    "    return max(abs(a-b) for a,b in zip(A,B))\n",
    "\n",
    "def cor_sum(vec, r): # correlation sum\n",
    "    r\"\"\"\n",
    "        Compute the correlation sum given some radius r\n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(vec)\n",
    "    neighbor_counts = [\n",
    "        sum(ell_inf(x_i, x_j) <= r for x_j in vec) / n\n",
    "        for x_i in vec\n",
    "    ]\n",
    "\n",
    "    return sum(neighbor_counts) / n\n",
    "\n",
    "def approx_entropy(time_series, m, d, r): \n",
    "    \"\"\"\n",
    "        Estimate the approximate/discrete entropy of a time series (approximating the growth rate of r-balls). \n",
    "\n",
    "        Params: \n",
    "\n",
    "        time_series: 1d array_like \n",
    "        m, d: int\n",
    "            delay embedding parameters\n",
    "        r: float\n",
    "            radius determining the size of an r-ball\n",
    "    \"\"\"\n",
    "    \n",
    "    def phi(delay_dim, delay): \n",
    "        \"\"\"\n",
    "            Compute the log-average of r-balls assuming the delay embedding parameters result in a diffeomorphism \n",
    "            of the 1d time series from R^1 to R^d. \n",
    "        \"\"\"\n",
    "        X = delay_embed(time_series, delay_dim, delay)\n",
    "        n = len(X)\n",
    "        neighbor_counts = [\n",
    "            sum(ell_inf(x_i, x_j) <= r for x_j in X) / n\n",
    "            for x_i in X\n",
    "        ]\n",
    "        count = [c for c in neighbor_counts if c > 0]\n",
    "        return sum(math.log(c) for c in count) / n\n",
    "\n",
    "    return abs(phi(m+1, d) - phi(m, d))\n",
    "\n",
    "def cor_dim(time_series, m, d, r): # correlation dimension; easier to compute than minkowski dimension\n",
    "    \"\"\"\n",
    "        Estimate the correlation dimension of a time series\n",
    "    \"\"\"\n",
    "    X = delay_embed(time_series, m, d)\n",
    "    CS = cor_sum(X, r)\n",
    "\n",
    "    return math.log(CS) / math.log(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e805bd99-5d5f-4c67-91a3-4ea22da15493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def AutoCor_k(arr, k): \n",
    "#     n = len(arr)\n",
    "#     M = np.mean(arr)\n",
    "#     num, denom = 0, 0\n",
    "#     for i in range(0, n):\n",
    "#         denom+=(arr[i]-M)**2\n",
    "#     for i in range(0, n-k): \n",
    "#         num+=(arr[i]-M)*(arr[i+k]-M)\n",
    "\n",
    "#     return num/denom\n",
    "\n",
    "def acf(x, lag, unbiased=True): # autocorrelation function; used to determine the optimal time delay\n",
    "    r\"\"\"\n",
    "        Compute the autocorrelation of a signal given delay/lag k. Can be used to determine the optimal time delay. \n",
    "\n",
    "        Params:\n",
    "\n",
    "        x: 1d array_like input data\n",
    "        lag: integer \n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    x = x-x.mean()\n",
    "    n = x.size\n",
    "\n",
    "    num = np.dot(x[:n-lag], x[lag:])\n",
    "    denom = np.dot(x, x)\n",
    "\n",
    "    if unbiased: \n",
    "        num /= (n-lag)\n",
    "        denom /= n\n",
    "    else: \n",
    "        num /= n\n",
    "        denom /= n\n",
    "\n",
    "    return num / denom\n",
    "\n",
    "def plot_acf(x, max_lag, unbiased=True): \n",
    "    r\"\"\"\n",
    "        Plot ACF from delay/lag = 1 to max_lag k. Returns the list [acf_i] where i is the delay parameter. \n",
    "\n",
    "        Params:\n",
    "\n",
    "        max_lag: int\n",
    "    \"\"\"\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    acf_list = [acf(x, i, unbiased=unbiased) for i in range(max_lag)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    lags = range(max_lag)\n",
    "    ax.plot(lags, acf_list, 'bo-', markersize=4, linewidth=1)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "    ax.set_xlabel('Lag')\n",
    "    ax.set_ylabel('Autocorrelation')\n",
    "    ax.set_title('Autocorrelation Function over Lag')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "    ax.set_ylim(-1.2, 1.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return acf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47da6ce2-3f21-49c9-8f53-9ef3d075c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_hist(x, bins='fd', base=e): \n",
    "    r\"\"\"\n",
    "        Compute the probability distribution from histogram. Returns the probability and edges (from the hist). \n",
    "\n",
    "        Params: \n",
    "        \n",
    "        x: array-like input data\n",
    "        bins: see numpy documentation. fd: Freedman Diaconis estimator\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    edges = np.histogram_bin_edges(x, bins=bins)\n",
    "    counts, _ = np.histogram(x, bins=edges)\n",
    "    total = counts.sum()\n",
    "\n",
    "    if total == 0: \n",
    "        return np.array([1.0]), edges\n",
    "\n",
    "    p = counts.astype(float) / total\n",
    "\n",
    "    return p, edges\n",
    "\n",
    "def joint_p_hist(x, y, bins='fd', base=e, edge_x=None, edge_y=None):\n",
    "    r\"\"\"\n",
    "        Compute the joint probability distribution P_xy from histogram.\n",
    "\n",
    "        Params: \n",
    "\n",
    "        x, y: array_like input data\n",
    "        edge_x, edge_y: predefined edges of hist_x and hist_y. \n",
    "            Specify edges to ensure consistency. \n",
    "    \"\"\"\n",
    "\n",
    "    x = np.asarray(x, dtype=float).ravel()\n",
    "    y = np.asarray(y, dtype=float).ravel()\n",
    "\n",
    "    if edge_x is None: \n",
    "        edge_x = np.histogram_bin_edges(x, bins=bins)\n",
    "    if edge_y is None: \n",
    "        edge_y = np.histogram_bin_edges(y, bins=bins)\n",
    "\n",
    "    counts, _, _ = np.hisogram2d(x, y, bins=(edge_x, edge_y))\n",
    "    total = counts.sum()\n",
    "\n",
    "    if total == 0: \n",
    "        return np.array([[1.0]], edge_x, edge_y)\n",
    "\n",
    "    p = counts.astype(float) / total\n",
    "\n",
    "    return p, edge_x, edge_y\n",
    "\n",
    "def H_from_p(p, base=e): \n",
    "    r\"\"\"\n",
    "        Compute the shannon entropy given a probability distribution p. \n",
    "    \"\"\"\n",
    "    p = np.asarray(p, dtype=float)\n",
    "    p = p[p > 0]\n",
    "\n",
    "    if p.size == 0: \n",
    "        return 0.0\n",
    "        \n",
    "    return -np.sum(p * (np.log(p) / np.log(base)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c92819-1a26-4714-b8d5-61cb895a91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay_embed(time_series, embedding_dimension, time_delay):\n",
    "    r\"\"\"\n",
    "        Delay-embed a 1-dimensional seqeunce (time series). Returns R^{N-t(d-1) x d} delay embedding. \n",
    "\n",
    "        time_series: array-like input data\n",
    "        embedding_dimension (d), time_delay (tau): delay-embedding parameters. \n",
    "\n",
    "        Original 1-d data transformed as [[x_0, x_t, ..., x_t(d-1)], ....]\n",
    "    \"\"\"\n",
    "    N = len(time_series)\n",
    "    num_vecs = N - (embedding_dimension - 1) * time_delay\n",
    "    embedded = np.zeros((num_vecs, embedding_dimension))\n",
    "    for i in range(num_vecs):\n",
    "        for j in range(embedding_dimension):\n",
    "            embedded[i, j] = time_series[i + j * time_delay]\n",
    "\n",
    "    # delay_matrix = num_vecs**(-0.5)*embedded\n",
    "    \n",
    "    return embedded\n",
    "\n",
    "def rolling_cor(x, y, dim, delay): \n",
    "    r\"\"\"\n",
    "        Compute the rolling correlation between two delay-embedded time series. \n",
    "    \"\"\"\n",
    "    x_embed = delay_embed(x, dim, delay)\n",
    "    y_embed = delay_embed(y, dim, delay)\n",
    "    cors = []\n",
    "    for i in range(len(x_embed)): \n",
    "        cor_mat = np.corrcoef(x_embed[i], y_embed[i])\n",
    "        cors.append(cor_mat[0,1])\n",
    "\n",
    "    return np.array(cors)\n",
    "\n",
    "# Old --------\n",
    "def shannon_entropy(x, bins='fd', base=e, from_probs=False): \n",
    "    x = np.asarray(x, dtype=float)\n",
    "\n",
    "    if from_probs: \n",
    "        p = x/x.sum()\n",
    "    else: \n",
    "        edges = np.histogram_bin_edges(x, bins=bins)\n",
    "        counts, _ = np.histogram(x, bins=edges)\n",
    "        p = counts / counts.sum()\n",
    "\n",
    "    p = [p > 0]\n",
    "    H = -np.sum(p * (np.log(p) / np.log(base)))\n",
    "\n",
    "    return H\n",
    "\n",
    "def joint_entropy(x, y, bins='fd', base=e, from_probs=False):\n",
    "    x, y = np.asarray(x, dtype=float), np.asarray(y, dtype=float)\n",
    "\n",
    "    if prob_probs: \n",
    "        p_xy = x / x.sum()\n",
    "    else: \n",
    "        edges_x = np.histogram_bin_edges(x, bins=bins)\n",
    "        edges_y = np.histogram_bin_edges(y, bins=bins)\n",
    "        counts, _, _ = np.histogram2d(x, y, bins=[edges_x, edges_y]) \n",
    "        p_xy = counts / counts.sum()\n",
    "\n",
    "    p_xy = p_xy.flatten()\n",
    "    p_xy = p_xy[p_xy > 0]\n",
    "\n",
    "    H_xy = -np.sum(p_xy * (np.log(p_xy) / np.log(base)))\n",
    "    \n",
    "    return Hxy\n",
    "\n",
    "def conditional_entropy(x, y, bins='fd', base=e, from_probs=False): \n",
    "    H_xy = join_entropy(x, y, bins=bins, base=base, from_probs=from_probs)\n",
    "    H_y = shannon_entropy(y, bins=bins, base=base, from_probs=from_probs)\n",
    "\n",
    "    return H_xy - H_y\n",
    "\n",
    "def mutual_info(x, y, bins='fd', base=e, from_probs=False): \n",
    "    H_x = shannon_entropy(x, bins=bins, base=base, from_probs=from_probs)\n",
    "    H_y = shannon_entropy(y, bins=bins, base=base, from_probs=from_probs)\n",
    "    H_xy = join_entropy(x, y, bins=bins, base=base, from_probs=from_probs)\n",
    "\n",
    "    return H_x + H_y - H_xy\n",
    "\n",
    "    \n",
    "# mutual information estimate based on historgram\n",
    "def ami(x, max_lag, bins='fd', base=e):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    N = x.size\n",
    "    lags = np.arange(1, max_lag+1)\n",
    "    ami = np.empty_like(lags, dtype=float)\n",
    "\n",
    "    Hx = shannon_entropy(x, bins=bins, base=base)\n",
    "\n",
    "    for idx, tau in enumerate(lags): \n",
    "        x1, x2 = x[:-tau], x[tau:]\n",
    "        \n",
    "    return lags, ami\n",
    "# ------------\n",
    "\n",
    "def delayPCA(time_series, delay_dim, delay, pca_dim): \n",
    "    ts_emb = delay_embed(time_series, delay_dim, delay)\n",
    "    pca = PCA(pca_dim).fit(ts_emb)\n",
    "    ts_delaypca = pca.transform(ts_emb) \n",
    "\n",
    "    return ts_delaypca, pca\n",
    "\n",
    "def project_pca(data, pca, numcomps=2): \n",
    "    x_proj = (data-pca.mean_) @ pca.components_[0:numcomps].T\n",
    "    interval = np.arange(len(x_proj))\n",
    "    if numcomps == 2: \n",
    "        plt.scatter(x_proj[:,0], x_proj[:,1], c=interval, cmap='viridis')\n",
    "        plt.xlabel(\"PC 1\"); plt.ylabel(\"PC 2\")\n",
    "    elif numcomps == 3: \n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(x_proj[:,0], x_proj[:,1], x_proj[:,2], c=interval, cmap='viridis', **scatter_kw)\n",
    "        ax.set_xlabel(\"PC 1\"); ax.set_ylabel(\"PC 2\"); ax.set_zlabel(\"PC 3\")\n",
    "    plt.title(f\"Projection onto first {numcomps} PCs\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "        \n",
    "    return x_proj\n",
    "\n",
    "def update_delayPCA(oldata, newobs, dim_delay, delay, dim_pca): \n",
    "    # make sure dim_pca is less than dim_delay\n",
    "    # this is a horrible piece of code; remind myself to fix this\n",
    "    assert dim_delay > dim_pca, \"PCA dim must be less than delay dim\"\n",
    "    \n",
    "    pca = PCA(dim_pca)\n",
    "    oldata_emb = delay_embed(oldata, dim_delay, delay)\n",
    "    old_pca = pca.fit(oldata_emb) \n",
    "    \n",
    "    newdata = np.hstack((oldata, newobs))\n",
    "    newdata_emb = delay_embed(newdata, dim_delay, delay)\n",
    "    new_pca = pca.fit(newdata_emb)\n",
    "    newdata_delaypca = new_pca.transform(newdata_emb)\n",
    "    newdata_oldbasis = old_pca.transform(newdata_emb)\n",
    "    \n",
    "    return old_pca, new_pca, newdata_delaypca, newdata_oldbasis\n",
    "\n",
    "def return_data_stats(data, dim, delay, r): \n",
    "    mean, std, ent, cordim = np.mean(data), np.std(data), approx_entropy(data, dim, delay, r), cor_dim(data, dim, delay, r)\n",
    "    print(f\"Data Mean: {mean} Data Stdev: {std} \\nData Entropy: {ent} Data Correlation Dim: {cordim}\")\n",
    "\n",
    "    return mean, std, ent, cordim\n",
    "\n",
    "def pca_diff_stats(pca1, pca2, ncomps): \n",
    "    angdiff = cossim(pca1.components_[:ncomps], pca2.components_[:ncomps])\n",
    "    meandiff = np.abs(pca1.mean_ - pca2.mean_)\n",
    "    print(f\"Mean Difference: {meandiff} Cosine Similarity: {angdiff}\")\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.plot(pca1.explained_variance_, label=f'{pca1} Eigenvalues')\n",
    "    plt.plot(pca2.explained_variance_, label=f'{pca2} Eigenvalues')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return angdiff, meandiff\n",
    "\n",
    "def compare_pcas(pca1, pca2, k, X1=None, X2=None):\n",
    "    from scipy.linalg import subspace_angles\n",
    "    U1, U2 = pca1.components_[:k].T, pca2.components_[:k].T\n",
    "    ang = subspace_angles(U1, U2)\n",
    "    subspace_dist = np.linalg.norm(np.sin(ang))\n",
    "    var_gap = (pca1.explained_variance_ratio_[:k].cumsum() -\n",
    "               pca2.explained_variance_ratio_[:k].cumsum())[-1]\n",
    "    overlap_sv = np.linalg.svd(U1.T @ U2, compute_uv=False)\n",
    "\n",
    "    results = dict(\n",
    "        principal_angles_deg=np.degrees(ang),\n",
    "        grassmann_dist=subspace_dist,\n",
    "        cum_variance_gap=var_gap,\n",
    "        overlap_singular_vals=overlap_sv,\n",
    "    )\n",
    "    if X1 is not None and X2 is not None:\n",
    "        results[\"recon_MSE_1to2\"] = pca_cross_recon_loss(pca1, pca2, X2)\n",
    "        results[\"recon_MSE_2to1\"] = pca_cross_recon_loss(pca2, pca1, X1)\n",
    "    return results\n",
    "\n",
    "def pca_recon_loss(X, pca, k): \n",
    "    Z = pca.transform(X)\n",
    "    Z[:, k:] = 0\n",
    "    X_hat = pca.inverse_transform(Z)\n",
    "\n",
    "    mse = np.mean((X-X_hat)**2)\n",
    "    per_sample = ((X-X_hat)**2).mean(axis=1)\n",
    "    per_feature = ((X-X_hat)**2).mean(axis=0)\n",
    "    var_j = X.var(axis=0, ddof=0)\n",
    "    r2 = 1.0 - per_feature/var_j\n",
    "\n",
    "    return mse, r2, X_hat\n",
    "\n",
    "# needs to be fixed\n",
    "def pca_cross_recon_loss(X, pca_enc, pca_dec, k): \n",
    "    \"\"\"\n",
    "        Calculates cross reconstruction loss for PCA. If pca_enc, pca_dec come from the same affine PCA transformation\n",
    "        then it returns the regular reconstruction loss. They are related to each other by \n",
    "        Z = (X - mu)B and X = ZB^T + mu\n",
    "    \"\"\"\n",
    "    B_enc, mu_enc = pca_enc.components_[:k].T, pca_enc.mean_\n",
    "    B_dec, mu_dec = pca_dec.components_[:k].T, pca_dec.mean_\n",
    "\n",
    "    Z_enc = (X - mu_enc) @ B_enc\n",
    "\n",
    "    U, _, Vt = np.linalg.svd(B_enc.T @ B_dec, full_matrices = False)\n",
    "    R = U @ Vt\n",
    "\n",
    "    Z_dec = Z_enc @ R\n",
    "\n",
    "    X_hat = Z_dec @ B_dec.T + mu_dec\n",
    "\n",
    "    mse = np.mean((X-X_hat)**2)\n",
    "    per_sample = ((X-X_hat)**2).mean(axis=1)\n",
    "    per_feature = ((X-X_hat)**2).mean(axis=0)\n",
    "    var_j = X.var(axis=0, ddof=0)\n",
    "    r2 = 1.0 - per_feature/var_j\n",
    "\n",
    "    return mse, r2, X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3274bf75-5e17-4652-9069-c6b78cef445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nearest_nbr(data): \n",
    "#     n, d = data.shape\n",
    "#     M_idx = np.empty(N, dtype=int)\n",
    "#     M_dist = np.empty(N, dtype=float)\n",
    "#     for i in range(N):\n",
    "#         best_dist = np.inf\n",
    "#         best_idx = -1\n",
    "#         x_i = data[i]\n",
    "#         for j in range(N): \n",
    "#             if i == j: \n",
    "#                 continue\n",
    "#             dist = np.dot(x_i - data[j], x_i - data[j])\n",
    "#             if dist < best_dist: \n",
    "#                 best_dist = dist\n",
    "#                 best_idx = j\n",
    "#         M_idx[i] = best_idx\n",
    "#         M_dist[i] = best_dist ** 0.5\n",
    "\n",
    "#     return dist, M_idx \n",
    "\n",
    "# def nearest_nbr(X): \n",
    "#     N, d = X.shape\n",
    "#     D = np.full((N,N), np.inf)\n",
    "#     for i in range(N): \n",
    "#         for j in range(N): \n",
    "#             if i == j: \n",
    "#                 continue \n",
    "#             else: \n",
    "#                 D[i][j] = np.sqrt(np.dot(X[i]-X[j], X[i]-X[j])) \n",
    "                \n",
    "#     M_idx = np.argmin(D, axis=1)\n",
    "#     D_min = D[np.arange(N), M_idx]\n",
    "#     return D_min, M_idx\n",
    "\n",
    "def nearest_nbr(X): \n",
    "    diff = X[:, None, :] - X[None, :, :]\n",
    "    D2 = np.sum(diff*diff, axis=-1)\n",
    "\n",
    "    np.fill_diagonal(D2, np.inf)\n",
    "\n",
    "    idx_nn = np.argmin(D2, axis=1)\n",
    "    dists = np.sqrt(D2[np.arange(X.shape[0]), idx_nn])\n",
    "\n",
    "    return dists, idx_nn\n",
    "    \n",
    "def FalseNN(data, max_dim, delay, Atol, Rtol): # false nearest neighbor used to determine optimal embedding dimension\n",
    "    # assert len(data) > \n",
    "    data = np.asarray(data).ravel()\n",
    "    stdev = np.std(data)\n",
    "    if stdev == 0: \n",
    "        raise ValueError(\"Zero Standard Deviation\")\n",
    "\n",
    "    fnn_ptg = []\n",
    "    \n",
    "    for m in range(1, max_dim): \n",
    "        # emb_m = delay_embed(data, m, delay)\n",
    "        emb_m1 = delay_embed(data, m+1, delay)\n",
    "        emb_m = emb_m1[:, :-1]  \n",
    "\n",
    "        d_m, idx_nn = nearest_nbr(emb_m)\n",
    "\n",
    "        new_coord, new_coord_nn = emb_m1[:, -1], emb_m1[idx_nn, -1]\n",
    "        rel_growth = np.abs(new_coord - new_coord_nn) / d_m\n",
    "        cond_rel = rel_growth > Rtol\n",
    "\n",
    "        d_m1 = np.linalg.norm(emb_m1 - emb_m1[idx_nn], axis=1)\n",
    "        cond_abs = (d_m1/stdev) > Atol\n",
    "\n",
    "        fnn_ptg.append(100.0 * np.mean(cond_rel | cond_abs))\n",
    "        \n",
    "\n",
    "    return np.array(fnn_ptg)\n",
    "\n",
    "def lyap_exp(series, dim, delay, theiler_window=0, horizon=100, fs=1.0): \n",
    "    \"\"\"\n",
    "        Calculate the largest Laypunov exponent of a time series using Rosenstein. \n",
    "\n",
    "        Params-----\n",
    "\n",
    "        series (1d array_like): 1d input time series data\n",
    "        dim, delay (int): delay embedding params\n",
    "        theiler_window (int): window to exclude temporally close neighbors\n",
    "        horizon (int): Number of time steps to follow the divergence\n",
    "        fs (float): The sampling frequency of the series (in Hz)\n",
    "    \"\"\"\n",
    "    traj = delay_embed(series, dim, delay)\n",
    "    N = len(traj)\n",
    "\n",
    "    dist, idx = nearest_nbr(traj)\n",
    "\n",
    "    log_divergence = np.zeros((N, horizon))\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_idx = [i, 1]\n",
    "    if abs(i - nbr_idx) > theiler_window:\n",
    "        for k in range(horizon):\n",
    "            if i + k < N and neighbor_idx + k < N:\n",
    "                dist = np.linalg.norm(traj[i + k] - traj[nbr_idx + k])\n",
    "                if dist > 0:\n",
    "                    log_divergence[i, k] = np.log(dist)\n",
    "                else:\n",
    "                    log_divergence[i, k] = np.nan\n",
    "            else:\n",
    "                log_divergence[i, k] = np.nan\n",
    "\n",
    "    avg_log_divergence = np.nanmean(log_divergence, axis=0)\n",
    "    \n",
    "    time_steps = np.arange(horizon)\n",
    "    \n",
    "    valid_idx = ~np.isnan(avg_log_divergence)\n",
    "    if not np.any(valid_idx):\n",
    "        return np.nan, np.array([]), np.array([]) # Not enough data\n",
    "\n",
    "    fit = linregress(time_steps[valid_idx], avg_log_divergence[valid_idx])\n",
    "    \n",
    "    lyap_exp = fit.slope * fs\n",
    "\n",
    "    return lyap_exp, avg_log_divergence, time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072779a8-637b-4489-a161-7a5cdec3fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Multi-Layer Perceptron (MLP) with optional normalization and dropout.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input: int,\n",
    "        num_hidden: list,\n",
    "        num_output: int,\n",
    "        nonlinearity: str = \"relu\",\n",
    "        norm: str = None,\n",
    "        dropout: float = 0.0,\n",
    "        out_nonlinearity: str = None,\n",
    "        init: str = \"xavier\",\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define activation functions\n",
    "        activations = {\"relu\": nn.ReLU(), \"tanh\": nn.Tanh(), \"sigmoid\": nn.Sigmoid()}\n",
    "        self.nonlinearity = activations.get(nonlinearity)\n",
    "        \n",
    "        layers = []\n",
    "        current_dim = num_input\n",
    "        \n",
    "        # Hidden layers\n",
    "        for hidden_dim in num_hidden:\n",
    "            layers.append(nn.Linear(current_dim, hidden_dim, bias=bias))\n",
    "            \n",
    "            if norm == \"batch\":\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            elif norm == \"layer\":\n",
    "                layers.append(nn.LayerNorm(hidden_dim))\n",
    "                \n",
    "            # Correctly append the already-instantiated activation function\n",
    "            if self.nonlinearity:\n",
    "                layers.append(self.nonlinearity)\n",
    "            \n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            \n",
    "            current_dim = hidden_dim\n",
    "            \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(current_dim, num_output, bias=bias))\n",
    "        \n",
    "        if out_nonlinearity:\n",
    "            self.out_nonlinearity = activations.get(out_nonlinearity)\n",
    "            if self.out_nonlinearity:\n",
    "                layers.append(self.out_nonlinearity)\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        # Weight initialization\n",
    "        if init == \"xavier\":\n",
    "            self.net.apply(self._xavier_init)\n",
    "        elif init == \"kaiming\":\n",
    "            self.net.apply(self._kaiming_init)\n",
    "\n",
    "    def _xavier_init(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def _kaiming_init(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d28445b-7f97-4a18-b7d4-83918d63e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions: \n",
    "def plot_delay(data, tau): \n",
    "    x = data[:-tau]  \n",
    "    y = data[tau:]  \n",
    "    \n",
    "    plt.scatter(x, y, s=1, alpha=0.7)  \n",
    "    plt.xlabel(f'x(t)')\n",
    "    plt.ylabel(f'x(t+{tau})')\n",
    "    plt.title(f'Delay Embedding (Ï„={tau})')\n",
    "    plt.show()\n",
    "\n",
    "# def L2_matrix(X):\n",
    "#     X_norm = np.sum(X**2, axis=1, keepdims=True)\n",
    "    \n",
    "#     return np.sqrt(X_norm + X_norm.T - 2 * X @ X.T)\n",
    "\n",
    "def L2_matrix(X):\n",
    "    X = np.asarray(X, dtype=float)\n",
    "    if X.ndim == 1:\n",
    "        X = X[:, None]                      \n",
    "    Xn = np.sum(X*X, axis=1)\n",
    "    D2 = Xn[:, None] + Xn[None, :] - 2.0 * (X @ X.T)\n",
    "    np.maximum(D2, 0, out=D2) \n",
    "    \n",
    "    return np.sqrt(D2)\n",
    "    \n",
    "# good vlues for r: 0.1 * max(d(x_i, x_j)), 0.2 * stdev(X) where x_i is a delay vector \n",
    "# X in the def below is a 1d time series\n",
    "def recurrence_mat(X, r, dim=2, delay=1):\n",
    "    T = delay_embed(X, dim, delay)\n",
    "    D = L2_matrix(T)\n",
    "\n",
    "    R = (D < r).astype(int)\n",
    "\n",
    "    return R\n",
    "\n",
    "def plot_recurrence(X, r, dim, delay, title=\"Recurrence Plot\"):\n",
    "    R = recurrence_mat(X, r, dim=dim, delay=delay)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(R, cmap='binary', origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time i')\n",
    "    plt.ylabel('Time j')\n",
    "    plt.colorbar(label='Recurrence')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def calculate_pca_loss_to_dim(X, pca, dim_min, dim_max): \n",
    "    pca_recon_loss_list = []\n",
    "    for i in range(dim_min, dim_max+1): \n",
    "        loss, _, _ = pca_recon_loss(X, pca, i)\n",
    "        pca_recon_loss_list.append(loss)\n",
    "\n",
    "    return pca_recon_loss_list\n",
    "\n",
    "def calculate_pca_feature_loss_to_dim(X, pca, dim_min, dim_max): \n",
    "    N = dim_max - dim_min + 1\n",
    "    k = X.shape[1]\n",
    "    feature_loss_mat = np.zeros((N, k))\n",
    "    for i in range(N): \n",
    "        _, feature_loss, _ = pca_recon_loss(X, pca, i+dim_min)\n",
    "        feature_loss_mat[i] = feature_loss\n",
    "\n",
    "    return feature_loss_mat\n",
    "\n",
    "def plot_pca_recon_loss(X, pca, dim_min, dim_max): \n",
    "    res = calculate_pca_loss_to_dim(X, pca, dim_min, dim_max)\n",
    "\n",
    "    plt.title(\"PCA Reconstruction Loss (MSE) vs. Dimension\")\n",
    "    plt.xlabel(\"Dimension\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.plot(np.arange(dim_min, dim_max+1), res, '.')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a34ef4c-dfc8-45ed-86c1-e7657279f65f",
   "metadata": {},
   "source": [
    "### Loss and Error Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6159436b-07f9-440f-965b-954c95b1eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pe(y, y_pred): \n",
    "    y, y_pred = np.asarray(y), np.asarray(y_pred)\n",
    "    return (y_pred - y) / y\n",
    "\n",
    "def ape(y, y_pred): \n",
    "    y, y_pred = np.asarray(y), np.asarray(y_pred)\n",
    "    return np.abs(y_pred - y) / np.abs(y)\n",
    "\n",
    "def squared_error(y, y_pred): \n",
    "    y, y_pred = np.asarray(y), np.asarray(y_pred)\n",
    "    return (y_pred - y)**2\n",
    "\n",
    "def MAPE(y, y_pred): \n",
    "    return np.mean(ape(y, y_pred))\n",
    "\n",
    "def MSE(y, y_pred): \n",
    "    return np.mean(squared_error(y, y_pred))\n",
    "\n",
    "class PredError:\n",
    "    \"\"\"\n",
    "    A class to encapsulate prediction error calculations and plotting.\n",
    "    \"\"\"\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Initializes the class and calculates all relevant error metrics.\n",
    "        \"\"\"\n",
    "        self.y_true = np.asarray(y_true)\n",
    "        self.y_pred = np.asarray(y_pred)\n",
    "\n",
    "        # This private method is called to compute all errors at once\n",
    "        self._calculate_all_errors()\n",
    "\n",
    "    def _calculate_all_errors(self):\n",
    "        \"\"\"Calculates pointwise and aggregate errors.\"\"\"\n",
    "        # Pointwise errors (for plotting)\n",
    "        self.pe = pe(self.y_true, self.y_pred)\n",
    "        self.ape = ape(self.y_true, self.y_pred)\n",
    "        self.squared_error = squared_error(self.y_true, self.y_pred)\n",
    "        \n",
    "        # Aggregate errors (for reporting)\n",
    "        self.mape = MAPE(self.y_true, self.y_pred)\n",
    "        self.mse = MSE(self.y_true, self.y_pred)\n",
    "\n",
    "    def plot(self, error_type='squared'):\n",
    "        \"\"\"\n",
    "        Plots a specific type of pointwise error.\n",
    "        \n",
    "        Args:\n",
    "            error_type (str): The error to plot. Can be 'pe', 'ape', \n",
    "                              or 'squared_error'.\n",
    "        \"\"\"\n",
    "        error_data = {\n",
    "            'pe': self.pe,\n",
    "            'ape': self.ape,\n",
    "            'squared': self.squared_error\n",
    "        }\n",
    "        \n",
    "        titles = {\n",
    "            'pe': 'Pointwise Percentage Error (PE)',\n",
    "            'ape': 'Pointwise Absolute Percentage Error (APE)',\n",
    "            'squared': 'Pointwise Squared Error'\n",
    "        }\n",
    "\n",
    "        if error_type not in error_data:\n",
    "            valid_types = \", \".join(error_data.keys())\n",
    "            raise ValueError(f\"Invalid error_type '{error_type}'. Please choose from: {valid_types}\")\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(error_data[error_type])\n",
    "        plt.title(f\"{titles[error_type]} on Test Set\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(error_type.upper())\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad03fd-24cd-42e4-9255-34cf8f15dbae",
   "metadata": {},
   "source": [
    "## Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a51657a-4a26-4bec-86a8-4124db632522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_map(r, x_0, max_iter=1000):\n",
    "    \"\"\"Generates a time series using the logistic map equation.\"\"\"\n",
    "    traj = [x_0]\n",
    "    for i in range(1, max_iter):\n",
    "        x = r * traj[i-1] * (1 - traj[i-1])\n",
    "        traj.append(x)\n",
    "    return np.array(traj, dtype=np.float32)\n",
    "\n",
    "def laser_map(g, x_0, max_iter=1000):\n",
    "    \"\"\"Generates a time series using the logistic map equation.\"\"\"\n",
    "    traj = [x_0]\n",
    "    for i in range(1, max_iter):\n",
    "        x = g * traj[i-1] * (1 - np.tanh(traj[i-1]))\n",
    "        traj.append(x)\n",
    "    return np.array(traj, dtype=np.float32)\n",
    "\n",
    "def weierstrass(x, a, b, max_n):\n",
    "    \"\"\"Generates a time series using the Weierstrass function.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    res = np.zeros_like(x, dtype=np.float32)\n",
    "    for i in range(max_n):\n",
    "        res += a**i * np.cos(b**i * np.pi * x)\n",
    "    return res\n",
    "\n",
    "def lorenz(t, xyz, sigma=10, rho=28, beta=8/3):\n",
    "    x, y, z = xyz\n",
    "    dxdt = sigma * (y - x)\n",
    "    dydt = x * (rho - z) - y\n",
    "    dzdt = x * y - beta * z\n",
    "    return [dxdt, dydt, dzdt]\n",
    "\n",
    "def generate_lorenz(initial_state=[1.0, 0.0, 0.0], sigma=10.0, rho=28.0, beta=8.0/3.0, dt=0.01, num_steps=10000):\n",
    "\n",
    "    t_span = [0, num_steps * dt]\n",
    "    t_eval = np.arange(0.0, num_steps * dt, dt)\n",
    "\n",
    "    solution_obj = solve_ivp(\n",
    "        fun=lorenz, \n",
    "        t_span=t_span, \n",
    "        y0=initial_state, \n",
    "        args=(sigma, rho, beta),\n",
    "        t_eval=t_eval\n",
    "    )\n",
    "    \n",
    "    solution = solution_obj.y.T\n",
    "    \n",
    "    return solution\n",
    "\n",
    "def plot_lorenz_attractor(data):\n",
    "    fig = plt.figure(figsize=(12, 9))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    x = data[:, 0]\n",
    "    y = data[:, 1]\n",
    "    z = data[:, 2]\n",
    "    \n",
    "    ax.plot(x, y, z, lw=0.5, color='deepskyblue')\n",
    "    \n",
    "    # Set titles and labels for clarity\n",
    "    ax.set_title(\"Lorenz Attractor\", fontsize=16)\n",
    "    ax.set_xlabel(\"X Axis\")\n",
    "    ax.set_ylabel(\"Y Axis\")\n",
    "    ax.set_zlabel(\"Z Axis\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def mackey_glass_discrete(N=10000, tau=17, beta=0.2, gamma=0.1, n=10, x0=1.2):\n",
    "    size = N + tau + 1\n",
    "    x = np.empty(size, dtype=float)\n",
    "    x[:tau+1] = float(x0) \n",
    "\n",
    "    for t in range(tau, size - 1):\n",
    "        x_tau = x[t - tau]\n",
    "        x[t + 1] = (1.0 - gamma) * x[t] + beta * x_tau / (1.0 + x_tau**n)\n",
    "\n",
    "    return x[tau+1:]\n",
    "\n",
    "def double_sinusoid(x, phi=0.07, psi=np.sqrt(2)/10): \n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return np.sin(2*np.pi*phi*x) + 0.6*np.sin(2*np.pi*psi*x)\n",
    "\n",
    "def chirp(x, phi=0.02, psi=5*1e-5): \n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return np.sin(2*np.pi*(phi*x + psi/2*x**2))\n",
    "\n",
    "def generate_timeseries(key, length): \n",
    "    if key == 'logistic':\n",
    "        print(\"Using Logistic Map dataset.\")\n",
    "        series = logistic_map(r=3.8282, x_0=0.5, max_iter=length)\n",
    "    elif key == 'weierstrass':\n",
    "        print(\"Using Weierstrass Function dataset.\")\n",
    "        x_vals = np.linspace(-2, 2, length)\n",
    "        series = weierstrass(x=x_vals, a=0.5, b=3, max_n=15)\n",
    "    elif key == 'doublesine': \n",
    "        print(\"Using double sinusoid dataset.\")\n",
    "        x_vals = np.linspace(-1000, 1000, length)\n",
    "        series = double_sinusoid(x_vals)\n",
    "    elif key == 'chirp': \n",
    "        print(\"Using chirp dataset.\")\n",
    "        x_vals = np.linspace(-1000, 1000, length)\n",
    "        series = chirp(x_vals)\n",
    "    elif key == 'macglass': \n",
    "        print(\"Using discrete Mackey-Glass dataset.\")\n",
    "        series = mackey_glass_discrete(N=length, tau=17, beta=0.2, gamma=0.1, n=10, x0=1.2)\n",
    "    elif key == 'laser':\n",
    "        print(\"Using the laser-logistic map dataset.\")\n",
    "        series = laser_map(g=20, x0=0.5, max_iter=length)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset specified.\")\n",
    "        \n",
    "    return series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9f94387-56f9-4f30-a383-0cdc35623d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bifur_logistic(param_values, x0=0.5, iter_trans=1000, iter_plot=256):\n",
    "    \"\"\"\n",
    "        Plots the bifurcation diagram for logistic map.\n",
    "    \"\"\"\n",
    "    iter_total = iter_trans + iter_plot\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,8), dpi=150)\n",
    "\n",
    "    for r in param_values:\n",
    "        X = logistic_map(r, x0, iter_total)\n",
    "        attractors = X[iter_trans:]\n",
    "        plot_values_r = np.full(iter_plot, r)\n",
    "\n",
    "        ax.plot(plot_values_r, attractors,\n",
    "                marker=',', color='black', linestyle='none', alpha=0.5)\n",
    "\n",
    "    ax.set_title(\"Bifurcation Diagram of the Logistic Map\", fontsize=16)\n",
    "    ax.set_xlabel(\"Growth Rate Parameter (r)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Equilibrium Population (x)\", fontsize=12)\n",
    "    ax.set_xlim(param_values[0], param_values[-1])\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "def plot_bifur_laser(param_values, x0=0.5, iter_trans=1000, iter_plot=256):\n",
    "    \"\"\"\n",
    "        Plots the bifurcation diagram for logistic map.\n",
    "    \"\"\"\n",
    "    iter_total = iter_trans + iter_plot\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,8), dpi=150)\n",
    "\n",
    "    for r in param_values:\n",
    "        X = laser_map(r, x0, iter_total)\n",
    "        attractors = X[iter_trans:]\n",
    "        plot_values_r = np.full(iter_plot, r)\n",
    "\n",
    "        ax.plot(plot_values_r, attractors,\n",
    "                marker=',', color='black', linestyle='none', alpha=0.5)\n",
    "\n",
    "    ax.set_title(\"Bifurcation Diagram of the Logistic Map\", fontsize=16)\n",
    "    ax.set_xlabel(\"Growth Rate Parameter (r)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Equilibrium Population (x)\", fontsize=12)\n",
    "    ax.set_xlim(param_values[0], param_values[-1])\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
